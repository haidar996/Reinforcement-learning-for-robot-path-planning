# Reinforcement-learning-for-robot-path-planning
This project implements and compares multiple reinforcement learning algorithms for multi-robot path planning in grid-based environments with obstacles. The system enables robots to navigate from start positions to goal positions while avoiding collisions and obstacles using various RL approaches.
# ğŸ¤– Multi-Robot RL Path Planning

A reinforcement learning-based path planning system for multiple robots in a grid world.  
Implements classical and advanced RL algorithms with visualization tools and dynamic environment support.

---

## ğŸ”‘ Key Features

- **Multiple RL Algorithms**: Value Iteration, Q-Learning, Dyna-Q, Dyna-Q+
- **Multi-Robot Coordination**: Supports multiple robots with collision avoidance
- **Grid World Environment**: Customizable grid sizes and obstacles
- **Visualization Tools**: Animated robot paths and policy grid visualization
- **Dynamic Environment Support**: Dyna-Q+ handles changing environments

---

## ğŸ§  Algorithms Implemented

- `value_iteration.py` â€“ Model-based RL using dynamic programming  
- `Q_learning.py` â€“ Model-free, off-policy TD learning  
- `dyna_q.py` â€“ Combines real experience with model-based planning  
- `dyna_qplus.py` â€“ Enhanced Dyna-Q for dynamic environments

---

## ğŸš€ Getting Started

### âœ… Prerequisites

- Python 3.7+
- NumPy
- Matplotlib

### ğŸ’¾ Installation

```bash
git clone <your-repo-url>
cd multi-robot-rl-path-planning
pip install numpy matplotlib
â–¶ï¸ Usage
Run any of the algorithms to see path planning in action:

bash
Copy code
python value_iteration.py
python Q_learning.py
python dyna_q.py
python dyna_qplus.py
Each script will:

Initialize the grid world with obstacles

Train the RL agent(s)

Display the learned policy grid

Animate the robot(s)' path

Show final paths for all agents

ğŸ—‚ï¸ File Structure
bash
Copy code
â”œâ”€â”€ value_iteration.py     # Value Iteration algorithm
â”œâ”€â”€ Q_learning.py          # Q-Learning algorithm  
â”œâ”€â”€ dyna_q.py              # Dyna-Q algorithm
â”œâ”€â”€ dyna_qplus.py          # Dyna-Q+ algorithm
â””â”€â”€ README.md              # This file
âš™ï¸ Configuration
ğŸ”§ Environment Setup
Edit these parameters in each script:

python
Copy code
width = 10                # Grid width
height = 10               # Grid height
step = 1                  # Movement step size
list = []                 # Obstacle list [x, y, width, height, ...]
ğŸ¤– Robot Configuration
python
Copy code
# Define robots with start and end positions
robot1 = robot(x_start, y_start, x_end, y_end)
agents = [robot1, robot2, ...]  # Multiple agents
ğŸ“Š Results
Each algorithm provides:

Policy grids with optimal actions

Animated robot paths

Collision-free multi-robot coordination

Efficient obstacle-avoiding trajectories

ğŸ¨ Visualization
Grid world with gray obstacles

Start positions: ğŸŸ¢ â€˜Sâ€™

Goal positions: ğŸ”´ â€˜Gâ€™

Policy directions with arrows

Animated multi-robot movement

Color-coded robot paths

ğŸ”§ Customization
You can easily:

Change grid dimensions and obstacle layout

Adjust learning parameters: Î± (alpha), Î³ (gamma), Îµ (epsilon)

Modify rewards

Add more robots

Test new obstacle configurations

ğŸ“ˆ Performance Summary
Value Iteration: Fast, optimal with known models

Q-Learning: Good for unknown environments

Dyna-Q: Efficient with both learning and planning

Dyna-Q+: Adapts to dynamic, changing environments

ğŸ¤ Contributing
Contributions are welcome! You can:

Add new RL algorithms

Improve visualization

Optimize code performance

Enhance environment features

ğŸ“„ License
This project is open source and available under the MIT License.
